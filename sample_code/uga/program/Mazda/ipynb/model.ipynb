{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48bd5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model test 用\n",
    "\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f6d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my vit\n",
    "\n",
    "import copy\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 12, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        # print(\"Attention_out:\", self.to_out(out).shape)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, fc_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, fc_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, in_size, num_obs, out_size, dim, depth, heads, fc_dim, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        position = PositionalEncoding(dim, dropout)\n",
    "        c = copy.deepcopy\n",
    "        self.input_embedding = nn.Sequential(LinearEmbedding(in_size,dim), c(position))\n",
    "        \n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_obs + 12, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 12, dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, fc_dim, dropout)\n",
    "\n",
    "        #self.pool = pool\n",
    "        self.generator = Generator(dim, out_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # print(\"input:\", input.shape)\n",
    "        x = self.input_embedding(input)\n",
    "        b, n, _ = x.shape\n",
    "        # print(\"in_en:\", x.shape)\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 12)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # print(\"out_TF:\", x.shape)\n",
    "\n",
    "        #x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.generator(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement the PE function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"pos_x:\", x.shape)\n",
    "        # print(\"pos_e:\", Variable(self.pe[:, :x.size(1)], requires_grad=False).shape)\n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class LinearEmbedding(nn.Module):\n",
    "    def __init__(self, inp_size,d_model):\n",
    "        super(LinearEmbedding, self).__init__()\n",
    "        # lut => lookup table\n",
    "        self.lut = nn.Linear(inp_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "\n",
    "    def __init__(self, dim, out_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, out_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"generator:\", x.shape)\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c44de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original vit\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 20, dim_head = 1600, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, fc_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, fc_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, num_pred, dim, depth, heads = 20, fc_dim, dim_head = 1600, dropout = 0.1, emb_dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        x_dim = 40\n",
    "        y_dim = 40\n",
    "        z_dim = 20\n",
    "        \n",
    "        patch_dim = x_dim * y_dim\n",
    "        \n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            reshape = Rearrange('b x y z data -> b (x y) z data', x = x_dim, y_dim = 40, z_dim = 20)\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "        \n",
    "        self.num_pred = num_pred\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, patch_dim + num_pred, dim))\n",
    "        self.pred_token = nn.Parameter(torch.randn(1, num_pred, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, fc_dim, dropout)\n",
    "\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_pred)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.to_patch_embedding(data)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((pred_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + num_pred)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        return self.generator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66edc429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: torch.Size([1, 40, 40, 20, 12])\n",
      "patch_reshape: torch.Size([1, 20, 19200])\n",
      "patch_emb: torch.Size([1, 20, 512])\n",
      "pred_token: torch.Size([1, 101, 512])\n",
      "pred_tokens: torch.Size([1, 101, 512])\n",
      "cat(pred_tokens,x): torch.Size([1, 121, 512])\n",
      "pos_emb: torch.Size([1, 121, 512])\n",
      "pos_dropout: torch.Size([1, 121, 512])\n",
      "PreNorm_Attn torch.Size([1, 121, 512])\n",
      "qkv: torch.Size([1, 121, 30720])\n",
      "qkv[0]: torch.Size([1, 121, 10240])\n",
      "qkv[1]: torch.Size([1, 121, 10240])\n",
      "qkv[2]: torch.Size([1, 121, 10240])\n",
      "q: torch.Size([1, 20, 121, 512])\n",
      "k: torch.Size([1, 20, 121, 512])\n",
      "v: torch.Size([1, 20, 121, 512])\n",
      "dots: torch.Size([1, 20, 121, 121])\n",
      "attn: torch.Size([1, 20, 121, 121])\n",
      "attn_score: torch.Size([1, 20, 121, 512])\n",
      "attn_reshape torch.Size([1, 121, 10240])\n",
      "attn_Linear: torch.Size([1, 121, 512])\n",
      "attn_out: torch.Size([1, 121, 512])\n",
      "x_attn_out: torch.Size([1, 121, 512])\n",
      "PreNorm_Attn torch.Size([1, 121, 512])\n",
      "ff_Linear1: torch.Size([1, 121, 2048])\n",
      "ff_GELU: torch.Size([1, 121, 2048])\n",
      "ff_dropout1 torch.Size([1, 121, 2048])\n",
      "ff_Linear2: torch.Size([1, 121, 512])\n",
      "ff_dropout2 torch.Size([1, 121, 512])\n",
      "x_ff_out: torch.Size([1, 121, 512])\n",
      "pooling: torch.Size([1, 512])\n",
      "latent: torch.Size([1, 512])\n",
      "generator_norm torch.Size([1, 512])\n",
      "spec: torch.Size([1, 7])\n",
      "spec_tokens: torch.Size([1, 7])\n",
      "cat(spec_tokens,x): torch.Size([1, 519])\n",
      "model_out: torch.Size([1, 101])\n"
     ]
    }
   ],
   "source": [
    "# Mazda_? xy平面をパッチとして入力2\n",
    "\n",
    "inp = torch.rand(1,40,40,20,12)\n",
    "print(\"inp:\", inp.shape)\n",
    "\n",
    "x_dim = 40\n",
    "y_dim = 40\n",
    "z_dim = 20\n",
    "data_dim = 12\n",
    "\n",
    "patch_dim = x_dim * y_dim * data_dim\n",
    "\n",
    "patch_reshape = Rearrange('b x y z data -> b z (x y data)', x = x_dim, y = y_dim, z = z_dim, data = data_dim)\n",
    "x = patch_reshape(inp)\n",
    "print(\"patch_reshape:\", x.shape)\n",
    "\n",
    "dim = 512\n",
    "\n",
    "patch_L = nn.Linear(patch_dim, dim)\n",
    "x = patch_L(x)\n",
    "print(\"patch_emb:\", x.shape)\n",
    "\n",
    "#for num_z in range(z_dim):\n",
    "#    if num_z == 0:\n",
    "#        patch_x = patch_L(x[:,:,num_z,:])\n",
    "#    else:\n",
    "#        patch_x = torch.cat((patch_x, patch_L(x[:,:,num_z,:])), dim=1)\n",
    "#print(\"patch_Linear:\", patch_x.shape)\n",
    "\n",
    "b, n, _ = x.shape\n",
    "num_pred = 101\n",
    "\n",
    "pred_token = nn.Parameter(torch.randn(1, num_pred, dim))\n",
    "print(\"pred_token:\", pred_token.shape)\n",
    "\n",
    "pred_tokens = repeat(pred_token, '() n d -> b n d', b = b)\n",
    "print(\"pred_tokens:\", pred_tokens.shape)\n",
    "\n",
    "x = torch.cat((pred_tokens, x), dim=1)\n",
    "print(\"cat(pred_tokens,x):\", x.shape)\n",
    "\n",
    "pos_emb = nn.Parameter(torch.randn(1, patch_dim + num_pred, dim))\n",
    "x += pos_emb[:,:(n+num_pred)]\n",
    "print(\"pos_emb:\", x.shape)\n",
    "\n",
    "dropout_rate = 0.1\n",
    "pos_dropout = nn.Dropout(dropout_rate)\n",
    "x = pos_dropout(x)\n",
    "print(\"pos_dropout:\", x.shape)\n",
    "\n",
    "attn_norm = nn.LayerNorm(dim)\n",
    "attn_in = attn_norm(x)\n",
    "print(\"PreNorm_Attn\", attn_in.shape)\n",
    "\n",
    "dim_head = 512\n",
    "heads = 20\n",
    "inner_dim = dim_head * heads\n",
    "\n",
    "scale = dim_head ** -0.5\n",
    "\n",
    "to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "print(\"qkv:\", to_qkv(x).shape)\n",
    "qkv = to_qkv(x).chunk(3, dim = -1)\n",
    "print(\"qkv[0]:\", qkv[0].shape)\n",
    "print(\"qkv[1]:\", qkv[1].shape)\n",
    "print(\"qkv[2]:\", qkv[2].shape)\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = heads), qkv)\n",
    "print(\"q:\", q.shape)\n",
    "print(\"k:\", k.shape)\n",
    "print(\"v:\", v.shape)\n",
    "\n",
    "dots = torch.matmul(q, k.transpose(-1, -2)) * scale\n",
    "print(\"dots:\", dots.shape)\n",
    "\n",
    "attend = nn.Softmax(dim = -1)\n",
    "attn = attend(dots)\n",
    "print(\"attn:\", attn.shape)\n",
    "\n",
    "attn_score = torch.matmul(attn, v)\n",
    "print(\"attn_score:\", attn_score.shape)\n",
    "\n",
    "attn_score = rearrange(attn_score, 'b h n d -> b n (h d)')\n",
    "print(\"attn_reshape\", attn_score.shape)\n",
    "\n",
    "attn_L = nn.Linear(inner_dim, dim)\n",
    "attn_out = attn_L(attn_score)\n",
    "print(\"attn_Linear:\", attn_out.shape)\n",
    "\n",
    "attn_dropout = nn.Dropout(dropout_rate)\n",
    "attn_out = attn_dropout(attn_out)\n",
    "print(\"attn_out:\", attn_out.shape)\n",
    "\n",
    "x = attn_out + x\n",
    "print(\"x_attn_out:\", x.shape)\n",
    "\n",
    "ff_norm = nn.LayerNorm(dim)\n",
    "ff_in = ff_norm(x)\n",
    "print(\"PreNorm_Attn\", ff_in.shape)\n",
    "\n",
    "hidden_dim = 2048\n",
    "ff_L1 = nn.Linear(dim, hidden_dim)\n",
    "ff = ff_L1(ff_in)\n",
    "print(\"ff_Linear1:\", ff.shape)\n",
    "\n",
    "GELU = nn.GELU()\n",
    "ff = GELU(ff)\n",
    "print(\"ff_GELU:\", ff.shape)\n",
    "\n",
    "ff_dropout1 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout1(ff)\n",
    "print(\"ff_dropout1\", ff.shape)\n",
    "\n",
    "ff_L2 = nn.Linear(hidden_dim, dim)\n",
    "ff = ff_L2(ff)\n",
    "print(\"ff_Linear2:\", ff.shape)\n",
    "\n",
    "ff_dropout2 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout2(ff)\n",
    "print(\"ff_dropout2\", ff.shape)\n",
    "\n",
    "x = ff + x\n",
    "print(\"x_ff_out:\", x.shape)\n",
    "\n",
    "x = x.mean(dim=1)\n",
    "print(\"pooling:\", x.shape)\n",
    "\n",
    "to_latent = nn.Identity()\n",
    "x = to_latent(x)\n",
    "print(\"latent:\", x.shape)\n",
    "\n",
    "generator_norm = nn.LayerNorm(dim)\n",
    "x = generator_norm(x)\n",
    "print(\"generator_norm\", x.shape)\n",
    "\n",
    "dim_spec = 7\n",
    "spec = torch.randn(1, dim_spec)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "spec_tokens = repeat(spec, '() d -> b d', b = b)\n",
    "print(\"spec_tokens:\", spec_tokens.shape)\n",
    "\n",
    "x = torch.cat((spec_tokens, x), dim=1)\n",
    "print(\"cat(spec_tokens,x):\", x.shape)\n",
    "\n",
    "generator = nn.Linear(dim+dim_spec, num_pred)\n",
    "pred = generator(x)\n",
    "print(\"model_out:\", pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c3055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: torch.Size([1, 40, 40, 20, 12])\n",
      "patch_reshape: torch.Size([1, 32000, 12])\n",
      "patch_Linear: torch.Size([1, 32000, 512])\n",
      "pred_token: torch.Size([1, 101, 512])\n",
      "pred_tokens: torch.Size([1, 101, 512])\n",
      "cat(pred_tokens,x): torch.Size([1, 32101, 512])\n",
      "pos_emb: torch.Size([1, 32101, 512])\n",
      "pos_dropout: torch.Size([1, 32101, 512])\n",
      "PreNorm_Attn torch.Size([1, 32101, 512])\n",
      "qkv: torch.Size([1, 32101, 30720])\n",
      "qkv[0]: torch.Size([1, 32101, 10240])\n",
      "qkv[1]: torch.Size([1, 32101, 10240])\n",
      "qkv[2]: torch.Size([1, 32101, 10240])\n",
      "q: torch.Size([1, 20, 32101, 512])\n",
      "k: torch.Size([1, 20, 32101, 512])\n",
      "v: torch.Size([1, 20, 32101, 512])\n"
     ]
    }
   ],
   "source": [
    "# Mazda_? 変数を1次元に並べる\n",
    "\n",
    "inp = torch.rand(1,40,40,20,12)\n",
    "print(\"inp:\", inp.shape)\n",
    "\n",
    "x_dim = 40\n",
    "y_dim = 40\n",
    "z_dim = 20\n",
    "data_dim = 12\n",
    "\n",
    "patch_dim = x_dim * y_dim * z_dim\n",
    "\n",
    "patch_reshape = Rearrange('b x y z data -> b (x y z) data', x = x_dim, y = y_dim, z = z_dim, data = data_dim)\n",
    "x = patch_reshape(inp)\n",
    "print(\"patch_reshape:\", x.shape)\n",
    "\n",
    "dim = 512\n",
    "\n",
    "patch_L = nn.Linear(data_dim, dim)\n",
    "x = patch_L(x)\n",
    "print(\"patch_Linear:\", x.shape)\n",
    "\n",
    "b, n, _ = x.shape\n",
    "num_pred = 101\n",
    "\n",
    "pred_token = nn.Parameter(torch.randn(1, num_pred, dim))\n",
    "print(\"pred_token:\", pred_token.shape)\n",
    "\n",
    "pred_tokens = repeat(pred_token, '() n d -> b n d', b = b)\n",
    "print(\"pred_tokens:\", pred_tokens.shape)\n",
    "\n",
    "x = torch.cat((pred_tokens, x), dim=1)\n",
    "print(\"cat(pred_tokens,x):\", x.shape)\n",
    "\n",
    "pos_emb = nn.Parameter(torch.randn(1, patch_dim + num_pred, dim))\n",
    "x += pos_emb[:,:(n+num_pred)]\n",
    "print(\"pos_emb:\", x.shape)\n",
    "\n",
    "dropout_rate = 0.1\n",
    "pos_dropout = nn.Dropout(dropout_rate)\n",
    "x = pos_dropout(x)\n",
    "print(\"pos_dropout:\", x.shape)\n",
    "\n",
    "attn_norm = nn.LayerNorm(dim)\n",
    "attn_in = attn_norm(x)\n",
    "print(\"PreNorm_Attn\", attn_in.shape)\n",
    "\n",
    "dim_head = 512\n",
    "heads = 20\n",
    "inner_dim = dim_head * heads\n",
    "\n",
    "scale = dim_head ** -0.5\n",
    "\n",
    "to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "print(\"qkv:\", to_qkv(x).shape)\n",
    "qkv = to_qkv(x).chunk(3, dim = -1)\n",
    "print(\"qkv[0]:\", qkv[0].shape)\n",
    "print(\"qkv[1]:\", qkv[1].shape)\n",
    "print(\"qkv[2]:\", qkv[2].shape)\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = heads), qkv)\n",
    "print(\"q:\", q.shape)\n",
    "print(\"k:\", k.shape)\n",
    "print(\"v:\", v.shape)\n",
    "\n",
    "dots = torch.matmul(q, k.transpose(-1, -2)) * scale\n",
    "print(\"dots:\", dots.shape)\n",
    "\n",
    "attend = nn.Softmax(dim = -1)\n",
    "attn = attend(dots)\n",
    "print(\"attn:\", attn.shape)\n",
    "\n",
    "attn_score = torch.matmul(attn, v)\n",
    "print(\"attn_score:\", attn_score.shape)\n",
    "\n",
    "attn_score = rearrange(attn_score, 'b h n d -> b n (h d)')\n",
    "print(\"attn_reshape\", attn_score.shape)\n",
    "\n",
    "attn_L = nn.Linear(inner_dim, dim)\n",
    "attn_out = attn_L(attn_score)\n",
    "print(\"attn_Linear:\", attn_out.shape)\n",
    "\n",
    "attn_dropout = nn.Dropout(dropout_rate)\n",
    "attn_out = attn_dropout(attn_out)\n",
    "print(\"attn_out:\", attn_out.shape)\n",
    "\n",
    "x = attn_out + x\n",
    "print(\"x_attn_out:\", x.shape)\n",
    "\n",
    "ff_norm = nn.LayerNorm(dim)\n",
    "ff_in = ff_norm(x)\n",
    "print(\"PreNorm_Attn\", ff_in.shape)\n",
    "\n",
    "hidden_dim = 2048\n",
    "ff_L1 = nn.Linear(dim, hidden_dim)\n",
    "ff = ff_L1(ff_in)\n",
    "print(\"ff_Linear1:\", ff.shape)\n",
    "\n",
    "GELU = nn.GELU()\n",
    "ff = GELU(ff)\n",
    "print(\"ff_GELU:\", ff.shape)\n",
    "\n",
    "ff_dropout1 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout1(ff)\n",
    "print(\"ff_dropout1\", ff.shape)\n",
    "\n",
    "ff_L2 = nn.Linear(hidden_dim, dim)\n",
    "ff = ff_L2(ff)\n",
    "print(\"ff_Linear2:\", ff.shape)\n",
    "\n",
    "ff_dropout2 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout2(ff)\n",
    "print(\"ff_dropout2\", ff.shape)\n",
    "\n",
    "x = ff + x\n",
    "print(\"x_ff_out:\", x.shape)\n",
    "\n",
    "x = x.mean(dim=1)\n",
    "print(\"pooling:\", x.shape)\n",
    "\n",
    "to_latent = nn.Identity()\n",
    "x = to_latent(x)\n",
    "print(\"latent:\", x.shape)\n",
    "\n",
    "generator_norm = nn.LayerNorm(dim)\n",
    "x = generator_norm(x)\n",
    "print(\"generator_norm\", x.shape)\n",
    "\n",
    "dim_spec = 7\n",
    "spec = torch.randn(1, dim_spec)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "spec_tokens = repeat(spec, '() d -> b d', b = b)\n",
    "print(\"spec_tokens:\", spec_tokens.shape)\n",
    "\n",
    "x = torch.cat((spec_tokens, x), dim=1)\n",
    "print(\"cat(spec_tokens,x):\", x.shape)\n",
    "\n",
    "generator = nn.Linear(dim+dim_spec, num_pred)\n",
    "pred = generator(x)\n",
    "print(\"model_out:\", pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55f4687f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: torch.Size([2, 40, 40, 20, 12])\n",
      "patch_reshape: torch.Size([2, 32000, 12])\n",
      "patch_Linear: torch.Size([2, 32000, 512])\n",
      "pred_token: torch.Size([1, 101, 512])\n",
      "pred_tokens: torch.Size([2, 101, 512])\n",
      "cat(pred_tokens,x): torch.Size([2, 32101, 512])\n",
      "pos_emb: torch.Size([2, 32101, 512])\n",
      "pos_dropout: torch.Size([2, 32101, 512])\n",
      "PreNorm_Attn torch.Size([2, 32101, 512])\n",
      "qkv: torch.Size([2, 32101, 30720])\n",
      "qkv[0]: torch.Size([2, 32101, 10240])\n",
      "qkv[1]: torch.Size([2, 32101, 10240])\n",
      "qkv[2]: torch.Size([2, 32101, 10240])\n",
      "q: torch.Size([2, 20, 32101, 512])\n",
      "k: torch.Size([2, 20, 32101, 512])\n",
      "v: torch.Size([2, 20, 32101, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:68] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 164875872160 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15750/3035077221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"v:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mdots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dots:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:68] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 164875872160 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "# Mazda_? 変数を1次元に並べる\n",
    "\n",
    "inp = torch.rand(1,40,40,20,12)\n",
    "print(\"inp:\", inp.shape)\n",
    "\n",
    "x_dim = 40\n",
    "y_dim = 40\n",
    "z_dim = 20\n",
    "data_dim = 12\n",
    "\n",
    "patch_dim = x_dim * y_dim * z_dim\n",
    "\n",
    "patch_reshape = Rearrange('b x y z data -> b (x y z) data', x = x_dim, y = y_dim, z = z_dim, data = data_dim)\n",
    "x = patch_reshape(inp)\n",
    "print(\"patch_reshape:\", x.shape)\n",
    "\n",
    "dim = 512\n",
    "\n",
    "patch_L = nn.Linear(data_dim, dim)\n",
    "x = patch_L(x)\n",
    "print(\"patch_Linear:\", x.shape)\n",
    "\n",
    "b, n, _ = x.shape\n",
    "num_pred = 101\n",
    "\n",
    "pred_token = nn.Parameter(torch.randn(1, num_pred, dim))\n",
    "print(\"pred_token:\", pred_token.shape)\n",
    "\n",
    "pred_tokens = repeat(pred_token, '() n d -> b n d', b = b)\n",
    "print(\"pred_tokens:\", pred_tokens.shape)\n",
    "\n",
    "x = torch.cat((pred_tokens, x), dim=1)\n",
    "print(\"cat(pred_tokens,x):\", x.shape)\n",
    "\n",
    "pos_emb = nn.Parameter(torch.randn(1, patch_dim + num_pred, dim))\n",
    "x += pos_emb[:,:(n+num_pred)]\n",
    "print(\"pos_emb:\", x.shape)\n",
    "\n",
    "dropout_rate = 0.1\n",
    "pos_dropout = nn.Dropout(dropout_rate)\n",
    "x = pos_dropout(x)\n",
    "print(\"pos_dropout:\", x.shape)\n",
    "\n",
    "attn_norm = nn.LayerNorm(dim)\n",
    "attn_in = attn_norm(x)\n",
    "print(\"PreNorm_Attn\", attn_in.shape)\n",
    "\n",
    "dim_head = 512\n",
    "heads = 20\n",
    "inner_dim = dim_head * heads\n",
    "\n",
    "scale = dim_head ** -0.5\n",
    "\n",
    "to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "print(\"qkv:\", to_qkv(x).shape)\n",
    "qkv = to_qkv(x).chunk(3, dim = -1)\n",
    "print(\"qkv[0]:\", qkv[0].shape)\n",
    "print(\"qkv[1]:\", qkv[1].shape)\n",
    "print(\"qkv[2]:\", qkv[2].shape)\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = heads), qkv)\n",
    "print(\"q:\", q.shape)\n",
    "print(\"k:\", k.shape)\n",
    "print(\"v:\", v.shape)\n",
    "\n",
    "dots = torch.matmul(q, k.transpose(-1, -2)) * scale\n",
    "print(\"dots:\", dots.shape)\n",
    "\n",
    "attend = nn.Softmax(dim = -1)\n",
    "attn = attend(dots)\n",
    "print(\"attn:\", attn.shape)\n",
    "\n",
    "attn_score = torch.matmul(attn, v)\n",
    "print(\"attn_score:\", attn_score.shape)\n",
    "\n",
    "attn_score = rearrange(attn_score, 'b h n d -> b n (h d)')\n",
    "print(\"attn_reshape\", attn_score.shape)\n",
    "\n",
    "attn_L = nn.Linear(inner_dim, dim)\n",
    "attn_out = attn_L(attn_score)\n",
    "print(\"attn_Linear:\", attn_out.shape)\n",
    "\n",
    "attn_dropout = nn.Dropout(dropout_rate)\n",
    "attn_out = attn_dropout(attn_out)\n",
    "print(\"attn_out:\", attn_out.shape)\n",
    "\n",
    "x = attn_out + x\n",
    "print(\"x_attn_out:\", x.shape)\n",
    "\n",
    "ff_norm = nn.LayerNorm(dim)\n",
    "ff_in = ff_norm(x)\n",
    "print(\"PreNorm_Attn\", ff_in.shape)\n",
    "\n",
    "hidden_dim = 2048\n",
    "ff_L1 = nn.Linear(dim, hidden_dim)\n",
    "ff = ff_L1(ff_in)\n",
    "print(\"ff_Linear1:\", ff.shape)\n",
    "\n",
    "GELU = nn.GELU()\n",
    "ff = GELU(ff)\n",
    "print(\"ff_GELU:\", ff.shape)\n",
    "\n",
    "ff_dropout1 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout1(ff)\n",
    "print(\"ff_dropout1\", ff.shape)\n",
    "\n",
    "ff_L2 = nn.Linear(hidden_dim, dim)\n",
    "ff = ff_L2(ff)\n",
    "print(\"ff_Linear2:\", ff.shape)\n",
    "\n",
    "ff_dropout2 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout2(ff)\n",
    "print(\"ff_dropout2\", ff.shape)\n",
    "\n",
    "x = ff + x\n",
    "print(\"x_ff_out:\", x.shape)\n",
    "\n",
    "x = x.mean(dim=1)\n",
    "print(\"pooling:\", x.shape)\n",
    "\n",
    "to_latent = nn.Identity()\n",
    "x = to_latent(x)\n",
    "print(\"latent:\", x.shape)\n",
    "\n",
    "generator_norm = nn.LayerNorm(dim)\n",
    "x = generator_norm(x)\n",
    "print(\"generator_norm\", x.shape)\n",
    "\n",
    "dim_spec = 7\n",
    "spec = torch.randn(1, dim_spec)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "spec_tokens = repeat(spec, '() d -> b d', b = b)\n",
    "print(\"spec_tokens:\", spec_tokens.shape)\n",
    "\n",
    "x = torch.cat((spec_tokens, x), dim=1)\n",
    "print(\"cat(spec_tokens,x):\", x.shape)\n",
    "\n",
    "generator = nn.Linear(dim+dim_spec, num_pred)\n",
    "pred = generator(x)\n",
    "print(\"model_out:\", pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height, image_width = pair(image_size)\n",
    "patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "patch_dim = channels * patch_height * patch_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f5e3395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: torch.Size([1, 40, 40, 20, 12])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b01df1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: torch.Size([1, 40, 40, 20, 12])\n",
      "x_dim:40, y_dim:40, z_dim:20\n",
      "patch_x:40, patch_y:40, patch_z:1\n",
      "patch_reshape: torch.Size([1, 20, 19200])\n",
      "patch_Linear: torch.Size([1, 20, 512])\n",
      "pred_token: torch.Size([1, 101, 512])\n",
      "pred_tokens: torch.Size([1, 101, 512])\n",
      "cat(pred_tokens,x): torch.Size([1, 121, 512])\n",
      "pos_emb: torch.Size([1, 121, 512])\n",
      "pos_dropout: torch.Size([1, 121, 512])\n",
      "PreNorm_Attn torch.Size([1, 121, 512])\n",
      "qkv: torch.Size([1, 121, 480])\n",
      "qkv[0]: torch.Size([1, 121, 160])\n",
      "qkv[1]: torch.Size([1, 121, 160])\n",
      "qkv[2]: torch.Size([1, 121, 160])\n",
      "q: torch.Size([1, 8, 121, 20])\n",
      "k: torch.Size([1, 8, 121, 20])\n",
      "v: torch.Size([1, 8, 121, 20])\n",
      "dots: torch.Size([1, 8, 121, 121])\n",
      "attn: torch.Size([1, 8, 121, 121])\n",
      "attn_score: torch.Size([1, 8, 121, 20])\n",
      "attn_reshape torch.Size([1, 121, 160])\n",
      "attn_Linear: torch.Size([1, 121, 512])\n",
      "attn_out: torch.Size([1, 121, 512])\n",
      "x_attn_out: torch.Size([1, 121, 512])\n",
      "PreNorm_ff torch.Size([1, 121, 512])\n",
      "ff_Linear1: torch.Size([1, 121, 2048])\n",
      "ff_GELU: torch.Size([1, 121, 2048])\n",
      "ff_dropout1 torch.Size([1, 121, 2048])\n",
      "ff_Linear2: torch.Size([1, 121, 512])\n",
      "ff_dropout2 torch.Size([1, 121, 512])\n",
      "x_ff_out: torch.Size([1, 121, 512])\n",
      "pooling: torch.Size([1, 512])\n",
      "latent: torch.Size([1, 512])\n",
      "generator_norm torch.Size([1, 512])\n",
      "spec: torch.Size([1, 7])\n",
      "spec_tokens: torch.Size([1, 7])\n",
      "cat(spec_tokens,x): torch.Size([1, 519])\n",
      "model_out: torch.Size([1, 101])\n"
     ]
    }
   ],
   "source": [
    "# パッチ分割\n",
    "\n",
    "patch_split_size = [1,1,20]\n",
    "\n",
    "inp = torch.rand(1,40,40,20,12)\n",
    "print(\"inp:\", inp.shape)\n",
    "\n",
    "_, x_dim, y_dim, z_dim, data_dim = inp.shape\n",
    "print(\"x_dim:%i, y_dim:%i, z_dim:%i\" % (x_dim, y_dim, z_dim))\n",
    "patch_x = x_dim // patch_split_size[0] \n",
    "patch_y = y_dim // patch_split_size[1]\n",
    "patch_z = z_dim // patch_split_size[2]\n",
    "print(\"patch_x:%i, patch_y:%i, patch_z:%i\" % (patch_x, patch_y, patch_z))\n",
    "\n",
    "num_patches = (x_dim // patch_x) * (y_dim // patch_y) * (z_dim // patch_z)\n",
    "\n",
    "patch_dim = patch_x * patch_y * patch_z * data_dim\n",
    "\n",
    "patch_reshape = Rearrange('b (x p1) (y p2) (z p3) data -> b (x y z) (p1 p2 p3 data)', p1=patch_x, p2=patch_y, p3=patch_z)  \n",
    "x = patch_reshape(inp)\n",
    "print(\"patch_reshape:\", x.shape)\n",
    "\n",
    "dim = 512\n",
    "\n",
    "patch_L = nn.Linear(patch_dim, dim)\n",
    "x = patch_L(x)\n",
    "print(\"patch_Linear:\", x.shape)\n",
    "\n",
    "b, n, _ = x.shape\n",
    "num_pred = 101\n",
    "\n",
    "pred_token = nn.Parameter(torch.randn(1, num_pred, dim))\n",
    "print(\"pred_token:\", pred_token.shape)\n",
    "\n",
    "pred_tokens = repeat(pred_token, '() n d -> b n d', b = b)\n",
    "print(\"pred_tokens:\", pred_tokens.shape)\n",
    "\n",
    "x = torch.cat((pred_tokens, x), dim=1)\n",
    "print(\"cat(pred_tokens,x):\", x.shape)\n",
    "\n",
    "pos_emb = nn.Parameter(torch.randn(1, num_patches + num_pred, dim))\n",
    "x += pos_emb[:,:(n+num_pred)]\n",
    "print(\"pos_emb:\", x.shape)\n",
    "\n",
    "dropout_rate = 0.1\n",
    "pos_dropout = nn.Dropout(dropout_rate)\n",
    "x = pos_dropout(x)\n",
    "print(\"pos_dropout:\", x.shape)\n",
    "\n",
    "attn_norm = nn.LayerNorm(dim)\n",
    "attn_in = attn_norm(x)\n",
    "print(\"PreNorm_Attn\", attn_in.shape)\n",
    "\n",
    "\n",
    "\n",
    "#dim_head = 512\n",
    "#heads = num_patches\n",
    "dim_head = num_patches\n",
    "heads = 8\n",
    "inner_dim = dim_head * heads\n",
    "\n",
    "scale = dim_head ** -0.5\n",
    "\n",
    "to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "print(\"qkv:\", to_qkv(x).shape)\n",
    "qkv = to_qkv(x).chunk(3, dim = -1)\n",
    "print(\"qkv[0]:\", qkv[0].shape)\n",
    "print(\"qkv[1]:\", qkv[1].shape)\n",
    "print(\"qkv[2]:\", qkv[2].shape)\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = heads), qkv)\n",
    "print(\"q:\", q.shape)\n",
    "print(\"k:\", k.shape)\n",
    "print(\"v:\", v.shape)\n",
    "\n",
    "dots = torch.matmul(q, k.transpose(-1, -2)) * scale\n",
    "print(\"dots:\", dots.shape)\n",
    "\n",
    "attend = nn.Softmax(dim = -1)\n",
    "attn = attend(dots)\n",
    "print(\"attn:\", attn.shape)\n",
    "\n",
    "attn_score = torch.matmul(attn, v)\n",
    "print(\"attn_score:\", attn_score.shape)\n",
    "\n",
    "attn_score = rearrange(attn_score, 'b h n d -> b n (h d)')\n",
    "print(\"attn_reshape\", attn_score.shape)\n",
    "\n",
    "attn_L = nn.Linear(inner_dim, dim)\n",
    "attn_out = attn_L(attn_score)\n",
    "print(\"attn_Linear:\", attn_out.shape)\n",
    "\n",
    "attn_dropout = nn.Dropout(dropout_rate)\n",
    "attn_out = attn_dropout(attn_out)\n",
    "print(\"attn_out:\", attn_out.shape)\n",
    "\n",
    "x = attn_out + x\n",
    "print(\"x_attn_out:\", x.shape)\n",
    "\n",
    "ff_norm = nn.LayerNorm(dim)\n",
    "ff_in = ff_norm(x)\n",
    "print(\"PreNorm_ff\", ff_in.shape)\n",
    "\n",
    "hidden_dim = 2048\n",
    "ff_L1 = nn.Linear(dim, hidden_dim)\n",
    "ff = ff_L1(ff_in)\n",
    "print(\"ff_Linear1:\", ff.shape)\n",
    "\n",
    "GELU = nn.GELU()\n",
    "ff = GELU(ff)\n",
    "print(\"ff_GELU:\", ff.shape)\n",
    "\n",
    "ff_dropout1 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout1(ff)\n",
    "print(\"ff_dropout1\", ff.shape)\n",
    "\n",
    "ff_L2 = nn.Linear(hidden_dim, dim)\n",
    "ff = ff_L2(ff)\n",
    "print(\"ff_Linear2:\", ff.shape)\n",
    "\n",
    "ff_dropout2 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout2(ff)\n",
    "print(\"ff_dropout2\", ff.shape)\n",
    "\n",
    "x = ff + x\n",
    "print(\"x_ff_out:\", x.shape)\n",
    "\n",
    "x = x.mean(dim=1)\n",
    "print(\"pooling:\", x.shape)\n",
    "\n",
    "to_latent = nn.Identity()\n",
    "x = to_latent(x)\n",
    "print(\"latent:\", x.shape)\n",
    "\n",
    "generator_norm = nn.LayerNorm(dim)\n",
    "x = generator_norm(x)\n",
    "print(\"generator_norm\", x.shape)\n",
    "\n",
    "dim_spec = 7\n",
    "spec = torch.randn(1, dim_spec)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "spec_tokens = repeat(spec, '() d -> b d', b = b)\n",
    "print(\"spec_tokens:\", spec_tokens.shape)\n",
    "\n",
    "x = torch.cat((spec_tokens, x), dim=1)\n",
    "print(\"cat(spec_tokens,x):\", x.shape)\n",
    "\n",
    "generator = nn.Linear(dim+dim_spec, num_pred)\n",
    "pred = generator(x)\n",
    "print(\"model_out:\", pred.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62fa6315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: torch.Size([1, 40, 40, 20, 12])\n",
      "x_dim:40, y_dim:40, z_dim:20\n",
      "patch_x:40, patch_y:40, patch_z:1\n",
      "patch_reshape: torch.Size([1, 20, 19200])\n",
      "patch_Linear: torch.Size([1, 20, 512])\n",
      "pred_token: torch.Size([1, 101, 512])\n",
      "pred_tokens: torch.Size([1, 101, 512])\n",
      "cat(pred_tokens,x): torch.Size([1, 121, 512])\n",
      "spec: torch.Size([1, 7])\n",
      "spec: torch.Size([1, 1, 7])\n",
      "spec: torch.Size([1, 1, 1, 7])\n",
      "spec1_Linear: torch.Size([1, 1, 512])\n",
      "cat(x, spec_1): torch.Size([1, 122, 512])\n",
      "spec2_Linear: torch.Size([1, 1, 1, 7])\n",
      "cat(x, spec_2): torch.Size([1, 123, 512])\n",
      "spec3_Linear: torch.Size([1, 1, 1, 7])\n",
      "cat(x, spec_3): torch.Size([1, 124, 512])\n",
      "spec4_Linear: torch.Size([1, 1, 1, 7])\n",
      "cat(x, spec_4): torch.Size([1, 125, 512])\n",
      "spec5_Linear: torch.Size([1, 1, 1, 7])\n",
      "cat(x, spec_5): torch.Size([1, 126, 512])\n",
      "spec_Linear: torch.Size([1, 1, 1, 7])\n",
      "cat(x, spec_6): torch.Size([1, 127, 512])\n",
      "spec_Linear: torch.Size([1, 1, 1, 7])\n",
      "cat(x, spec_7): torch.Size([1, 128, 512])\n",
      "pos_emb: torch.Size([1, 128, 512])\n",
      "pos_dropout: torch.Size([1, 128, 512])\n",
      "PreNorm_Attn torch.Size([1, 128, 512])\n",
      "qkv: torch.Size([1, 128, 480])\n",
      "qkv[0]: torch.Size([1, 128, 160])\n",
      "qkv[1]: torch.Size([1, 128, 160])\n",
      "qkv[2]: torch.Size([1, 128, 160])\n",
      "q: torch.Size([1, 8, 128, 20])\n",
      "k: torch.Size([1, 8, 128, 20])\n",
      "v: torch.Size([1, 8, 128, 20])\n",
      "dots: torch.Size([1, 8, 128, 128])\n",
      "attn: torch.Size([1, 8, 128, 128])\n",
      "attn_score: torch.Size([1, 8, 128, 20])\n",
      "attn_reshape torch.Size([1, 128, 160])\n",
      "attn_Linear: torch.Size([1, 128, 512])\n",
      "attn_out: torch.Size([1, 128, 512])\n",
      "x_attn_out: torch.Size([1, 128, 512])\n",
      "PreNorm_ff torch.Size([1, 128, 512])\n",
      "ff_Linear1: torch.Size([1, 128, 2048])\n",
      "ff_GELU: torch.Size([1, 128, 2048])\n",
      "ff_dropout1 torch.Size([1, 128, 2048])\n",
      "ff_Linear2: torch.Size([1, 128, 512])\n",
      "ff_dropout2 torch.Size([1, 128, 512])\n",
      "x_ff_out: torch.Size([1, 128, 512])\n",
      "pooling: torch.Size([1, 512])\n",
      "latent: torch.Size([1, 512])\n",
      "generator_norm torch.Size([1, 512])\n",
      "model_out: torch.Size([1, 101])\n"
     ]
    }
   ],
   "source": [
    "# パッチ分割 spec\n",
    "\n",
    "patch_split_size = [1,1,20]\n",
    "\n",
    "inp = torch.rand(1,40,40,20,12)\n",
    "print(\"inp:\", inp.shape)\n",
    "\n",
    "_, x_dim, y_dim, z_dim, data_dim = inp.shape\n",
    "print(\"x_dim:%i, y_dim:%i, z_dim:%i\" % (x_dim, y_dim, z_dim))\n",
    "patch_x = x_dim // patch_split_size[0] \n",
    "patch_y = y_dim // patch_split_size[1]\n",
    "patch_z = z_dim // patch_split_size[2]\n",
    "print(\"patch_x:%i, patch_y:%i, patch_z:%i\" % (patch_x, patch_y, patch_z))\n",
    "\n",
    "num_patches = (x_dim // patch_x) * (y_dim // patch_y) * (z_dim // patch_z)\n",
    "\n",
    "patch_dim = patch_x * patch_y * patch_z * data_dim\n",
    "\n",
    "patch_reshape = Rearrange('b (x p1) (y p2) (z p3) data -> b (x y z) (p1 p2 p3 data)', p1=patch_x, p2=patch_y, p3=patch_z)  \n",
    "x = patch_reshape(inp)\n",
    "print(\"patch_reshape:\", x.shape)\n",
    "\n",
    "dim = 512\n",
    "\n",
    "patch_L = nn.Linear(patch_dim, dim)\n",
    "x = patch_L(x)\n",
    "print(\"patch_Linear:\", x.shape)\n",
    "\n",
    "b, n, _ = x.shape\n",
    "\n",
    "num_pred = 101\n",
    "\n",
    "pred_token = nn.Parameter(torch.randn(1, num_pred, dim))\n",
    "print(\"pred_token:\", pred_token.shape)\n",
    "\n",
    "pred_tokens = repeat(pred_token, '() n d -> b n d', b = b)\n",
    "print(\"pred_tokens:\", pred_tokens.shape)\n",
    "\n",
    "x = torch.cat((pred_tokens, x), dim=1)\n",
    "print(\"cat(pred_tokens,x):\", x.shape)\n",
    "\n",
    "num_spec = 7\n",
    "spec = torch.randn(1, num_spec)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "\n",
    "spec = torch.unsqueeze(spec,0)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "spec = torch.unsqueeze(spec,0)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "\n",
    "spec_L1 = nn.Linear(1,dim)\n",
    "spec1 = spec_L1(spec[:,:,:,0])\n",
    "print(\"spec1_Linear:\", spec1.shape)\n",
    "\n",
    "x = torch.cat((x, spec1), dim=1)\n",
    "print(\"cat(x, spec_1):\", x.shape)\n",
    "\n",
    "\n",
    "spec_L2 = nn.Linear(1,dim)\n",
    "spec2 = spec_L2(spec[:,:,:,1])\n",
    "print(\"spec2_Linear:\", spec.shape)\n",
    "\n",
    "x = torch.cat((x, spec2), dim=1)\n",
    "print(\"cat(x, spec_2):\", x.shape)\n",
    "\n",
    "\n",
    "spec_L3 = nn.Linear(1,dim)\n",
    "spec3 = spec_L3(spec[:,:,:,2])\n",
    "print(\"spec3_Linear:\", spec.shape)\n",
    "\n",
    "x = torch.cat((x, spec3), dim=1)\n",
    "print(\"cat(x, spec_3):\", x.shape)\n",
    "\n",
    "\n",
    "spec_L4 = nn.Linear(1,dim)\n",
    "spec4 = spec_L4(spec[:,:,:,3])\n",
    "print(\"spec4_Linear:\", spec.shape)\n",
    "\n",
    "x = torch.cat((x, spec4), dim=1)\n",
    "print(\"cat(x, spec_4):\", x.shape)\n",
    "\n",
    "\n",
    "spec_L5 = nn.Linear(1,dim)\n",
    "spec5 = spec_L5(spec[:,:,:,4])\n",
    "print(\"spec5_Linear:\", spec.shape)\n",
    "\n",
    "x = torch.cat((x, spec5), dim=1)\n",
    "print(\"cat(x, spec_5):\", x.shape)\n",
    "\n",
    "\n",
    "spec_L6 = nn.Linear(1,dim)\n",
    "spec6 = spec_L6(spec[:,:,:,5])\n",
    "print(\"spec_Linear:\", spec.shape)\n",
    "\n",
    "x = torch.cat((x, spec6), dim=1)\n",
    "print(\"cat(x, spec_6):\", x.shape)\n",
    "\n",
    "\n",
    "spec_L7 = nn.Linear(1,dim)\n",
    "spec7 = spec_L7(spec[:,:,:,6])\n",
    "print(\"spec_Linear:\", spec.shape)\n",
    "\n",
    "x = torch.cat((x, spec7), dim=1)\n",
    "print(\"cat(x, spec_7):\", x.shape)\n",
    "\n",
    "\n",
    "pos_emb = nn.Parameter(torch.randn(1, num_patches + num_pred + num_spec, dim))\n",
    "x += pos_emb[:,:(n+num_pred+num_spec)]\n",
    "print(\"pos_emb:\", x.shape)\n",
    "\n",
    "dropout_rate = 0.1\n",
    "pos_dropout = nn.Dropout(dropout_rate)\n",
    "x = pos_dropout(x)\n",
    "print(\"pos_dropout:\", x.shape)\n",
    "\n",
    "attn_norm = nn.LayerNorm(dim)\n",
    "attn_in = attn_norm(x)\n",
    "print(\"PreNorm_Attn\", attn_in.shape)\n",
    "\n",
    "#dim_head = 512\n",
    "#heads = num_patches\n",
    "dim_head = num_patches\n",
    "heads = 8\n",
    "inner_dim = dim_head * heads\n",
    "\n",
    "scale = dim_head ** -0.5\n",
    "\n",
    "to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "print(\"qkv:\", to_qkv(x).shape)\n",
    "qkv = to_qkv(x).chunk(3, dim = -1)\n",
    "print(\"qkv[0]:\", qkv[0].shape)\n",
    "print(\"qkv[1]:\", qkv[1].shape)\n",
    "print(\"qkv[2]:\", qkv[2].shape)\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = heads), qkv)\n",
    "print(\"q:\", q.shape)\n",
    "print(\"k:\", k.shape)\n",
    "print(\"v:\", v.shape)\n",
    "\n",
    "dots = torch.matmul(q, k.transpose(-1, -2)) * scale\n",
    "print(\"dots:\", dots.shape)\n",
    "\n",
    "attend = nn.Softmax(dim = -1)\n",
    "attn = attend(dots)\n",
    "print(\"attn:\", attn.shape)\n",
    "\n",
    "attn_score = torch.matmul(attn, v)\n",
    "print(\"attn_score:\", attn_score.shape)\n",
    "\n",
    "attn_score = rearrange(attn_score, 'b h n d -> b n (h d)')\n",
    "print(\"attn_reshape\", attn_score.shape)\n",
    "\n",
    "attn_L = nn.Linear(inner_dim, dim)\n",
    "attn_out = attn_L(attn_score)\n",
    "print(\"attn_Linear:\", attn_out.shape)\n",
    "\n",
    "attn_dropout = nn.Dropout(dropout_rate)\n",
    "attn_out = attn_dropout(attn_out)\n",
    "print(\"attn_out:\", attn_out.shape)\n",
    "\n",
    "x = attn_out + x\n",
    "print(\"x_attn_out:\", x.shape)\n",
    "\n",
    "ff_norm = nn.LayerNorm(dim)\n",
    "ff_in = ff_norm(x)\n",
    "print(\"PreNorm_ff\", ff_in.shape)\n",
    "\n",
    "hidden_dim = 2048\n",
    "ff_L1 = nn.Linear(dim, hidden_dim)\n",
    "ff = ff_L1(ff_in)\n",
    "print(\"ff_Linear1:\", ff.shape)\n",
    "\n",
    "GELU = nn.GELU()\n",
    "ff = GELU(ff)\n",
    "print(\"ff_GELU:\", ff.shape)\n",
    "\n",
    "ff_dropout1 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout1(ff)\n",
    "print(\"ff_dropout1\", ff.shape)\n",
    "\n",
    "ff_L2 = nn.Linear(hidden_dim, dim)\n",
    "ff = ff_L2(ff)\n",
    "print(\"ff_Linear2:\", ff.shape)\n",
    "\n",
    "ff_dropout2 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout2(ff)\n",
    "print(\"ff_dropout2\", ff.shape)\n",
    "\n",
    "x = ff + x\n",
    "print(\"x_ff_out:\", x.shape)\n",
    "\n",
    "x = x.mean(dim=1)\n",
    "print(\"pooling:\", x.shape)\n",
    "\n",
    "to_latent = nn.Identity()\n",
    "x = to_latent(x)\n",
    "print(\"latent:\", x.shape)\n",
    "\n",
    "generator_norm = nn.LayerNorm(dim)\n",
    "x = generator_norm(x)\n",
    "print(\"generator_norm\", x.shape)\n",
    "\n",
    "generator = nn.Linear(dim, num_pred)\n",
    "pred = generator(x)\n",
    "print(\"model_out:\", pred.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d2c14c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: torch.Size([1, 40, 40, 20, 12])\n",
      "x_dim:40, y_dim:40, z_dim:20\n",
      "patch_x:40, patch_y:40, patch_z:1\n",
      "patch_reshape: torch.Size([1, 20, 19200])\n",
      "patch_Linear: torch.Size([1, 20, 512])\n",
      "pred_token: torch.Size([1, 101, 512])\n",
      "pred_tokens: torch.Size([1, 101, 512])\n",
      "cat(pred_tokens,x): torch.Size([1, 121, 512])\n",
      "spec: torch.Size([1, 7])\n",
      "spec: torch.Size([1, 1, 7])\n",
      "spec: torch.Size([1, 1, 1, 7])\n",
      "spec0_Linear:torch.Size([1, 1, 512])\n",
      "spec1_Linear:torch.Size([1, 2, 512])\n",
      "spec2_Linear:torch.Size([1, 3, 512])\n",
      "spec3_Linear:torch.Size([1, 4, 512])\n",
      "spec4_Linear:torch.Size([1, 5, 512])\n",
      "spec5_Linear:torch.Size([1, 6, 512])\n",
      "spec6_Linear:torch.Size([1, 7, 512])\n",
      "cat(x, spec_emb): torch.Size([1, 128, 512])\n",
      "pos_emb: torch.Size([1, 128, 512])\n",
      "pos_dropout: torch.Size([1, 128, 512])\n",
      "PreNorm_Attn torch.Size([1, 128, 512])\n",
      "qkv: torch.Size([1, 128, 480])\n",
      "qkv[0]: torch.Size([1, 128, 160])\n",
      "qkv[1]: torch.Size([1, 128, 160])\n",
      "qkv[2]: torch.Size([1, 128, 160])\n",
      "q: torch.Size([1, 8, 128, 20])\n",
      "k: torch.Size([1, 8, 128, 20])\n",
      "v: torch.Size([1, 8, 128, 20])\n",
      "dots: torch.Size([1, 8, 128, 128])\n",
      "attn: torch.Size([1, 8, 128, 128])\n",
      "attn_score: torch.Size([1, 8, 128, 20])\n",
      "attn_reshape torch.Size([1, 128, 160])\n",
      "attn_Linear: torch.Size([1, 128, 512])\n",
      "attn_out: torch.Size([1, 128, 512])\n",
      "x_attn_out: torch.Size([1, 128, 512])\n",
      "PreNorm_ff torch.Size([1, 128, 512])\n",
      "ff_Linear1: torch.Size([1, 128, 2048])\n",
      "ff_GELU: torch.Size([1, 128, 2048])\n",
      "ff_dropout1 torch.Size([1, 128, 2048])\n",
      "ff_Linear2: torch.Size([1, 128, 512])\n",
      "ff_dropout2 torch.Size([1, 128, 512])\n",
      "x_ff_out: torch.Size([1, 128, 512])\n",
      "pooling: torch.Size([1, 512])\n",
      "latent: torch.Size([1, 512])\n",
      "generator_norm torch.Size([1, 512])\n",
      "model_out: torch.Size([1, 101])\n"
     ]
    }
   ],
   "source": [
    "# パッチ分割 spec 完成版\n",
    "\n",
    "patch_split_size = [1,1,20]\n",
    "\n",
    "inp = torch.rand(1,40,40,20,12)\n",
    "print(\"inp:\", inp.shape)\n",
    "\n",
    "_, x_dim, y_dim, z_dim, data_dim = inp.shape\n",
    "print(\"x_dim:%i, y_dim:%i, z_dim:%i\" % (x_dim, y_dim, z_dim))\n",
    "patch_x = x_dim // patch_split_size[0] \n",
    "patch_y = y_dim // patch_split_size[1]\n",
    "patch_z = z_dim // patch_split_size[2]\n",
    "print(\"patch_x:%i, patch_y:%i, patch_z:%i\" % (patch_x, patch_y, patch_z))\n",
    "\n",
    "num_patches = (x_dim // patch_x) * (y_dim // patch_y) * (z_dim // patch_z)\n",
    "\n",
    "patch_dim = patch_x * patch_y * patch_z * data_dim\n",
    "\n",
    "patch_reshape = Rearrange('b (x p1) (y p2) (z p3) data -> b (x y z) (p1 p2 p3 data)', p1=patch_x, p2=patch_y, p3=patch_z)  \n",
    "x = patch_reshape(inp)\n",
    "print(\"patch_reshape:\", x.shape)\n",
    "\n",
    "dim = 512\n",
    "\n",
    "patch_L = nn.Linear(patch_dim, dim)\n",
    "x = patch_L(x)\n",
    "print(\"patch_Linear:\", x.shape)\n",
    "\n",
    "b, n, _ = x.shape\n",
    "\n",
    "num_pred = 101\n",
    "\n",
    "pred_token = nn.Parameter(torch.randn(1, num_pred, dim))\n",
    "print(\"pred_token:\", pred_token.shape)\n",
    "\n",
    "pred_tokens = repeat(pred_token, '() n d -> b n d', b = b)\n",
    "print(\"pred_tokens:\", pred_tokens.shape)\n",
    "\n",
    "x = torch.cat((pred_tokens, x), dim=1)\n",
    "print(\"cat(pred_tokens,x):\", x.shape)\n",
    "\n",
    "num_spec = 7\n",
    "spec = torch.randn(1, num_spec)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "spec = torch.unsqueeze(spec,1)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "spec = torch.unsqueeze(spec,1)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "def clones(module, n):\n",
    "    \"\"\"\n",
    "    Produce N identical layers.\n",
    "    \"\"\"\n",
    "    assert isinstance(module, nn.Module)\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(n)])\n",
    "\n",
    "spec_Ls = clones(nn.Linear(1,dim), num_spec)\n",
    "\n",
    "\n",
    "for i, spec_L in enumerate(spec_Ls):\n",
    "    if(i == 0):\n",
    "        spec_emb_all = spec_L(spec[:,:,:,i])\n",
    "        print(f'spec{i}_Linear:{spec_emb_all.shape}')\n",
    "    else:\n",
    "        spec_emb = spec_L(spec[:,:,:,i])\n",
    "        spec_emb_all = torch.cat((spec_emb_all, spec_emb), dim=1)\n",
    "        print(f'spec{i}_Linear:{spec_emb_all.shape}')\n",
    "\n",
    "\n",
    "x = torch.cat((x, spec_emb_all), dim=1)\n",
    "print(\"cat(x, spec_emb):\", x.shape)\n",
    "\n",
    "\n",
    "pos_emb = nn.Parameter(torch.randn(1, num_patches + num_pred + num_spec, dim))\n",
    "x += pos_emb[:,:(n+num_pred+num_spec)]\n",
    "print(\"pos_emb:\", x.shape)\n",
    "\n",
    "dropout_rate = 0.1\n",
    "pos_dropout = nn.Dropout(dropout_rate)\n",
    "x = pos_dropout(x)\n",
    "print(\"pos_dropout:\", x.shape)\n",
    "\n",
    "attn_norm = nn.LayerNorm(dim)\n",
    "attn_in = attn_norm(x)\n",
    "print(\"PreNorm_Attn\", attn_in.shape)\n",
    "\n",
    "#dim_head = 512\n",
    "#heads = num_patches\n",
    "dim_head = num_patches\n",
    "heads = 8\n",
    "inner_dim = dim_head * heads\n",
    "\n",
    "scale = dim_head ** -0.5\n",
    "\n",
    "to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "print(\"qkv:\", to_qkv(x).shape)\n",
    "qkv = to_qkv(x).chunk(3, dim = -1)\n",
    "print(\"qkv[0]:\", qkv[0].shape)\n",
    "print(\"qkv[1]:\", qkv[1].shape)\n",
    "print(\"qkv[2]:\", qkv[2].shape)\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = heads), qkv)\n",
    "print(\"q:\", q.shape)\n",
    "print(\"k:\", k.shape)\n",
    "print(\"v:\", v.shape)\n",
    "\n",
    "dots = torch.matmul(q, k.transpose(-1, -2)) * scale\n",
    "print(\"dots:\", dots.shape)\n",
    "\n",
    "attend = nn.Softmax(dim = -1)\n",
    "attn = attend(dots)\n",
    "print(\"attn:\", attn.shape)\n",
    "\n",
    "attn_score = torch.matmul(attn, v)\n",
    "print(\"attn_score:\", attn_score.shape)\n",
    "\n",
    "attn_score = rearrange(attn_score, 'b h n d -> b n (h d)')\n",
    "print(\"attn_reshape\", attn_score.shape)\n",
    "\n",
    "attn_L = nn.Linear(inner_dim, dim)\n",
    "attn_out = attn_L(attn_score)\n",
    "print(\"attn_Linear:\", attn_out.shape)\n",
    "\n",
    "attn_dropout = nn.Dropout(dropout_rate)\n",
    "attn_out = attn_dropout(attn_out)\n",
    "print(\"attn_out:\", attn_out.shape)\n",
    "\n",
    "x = attn_out + x\n",
    "print(\"x_attn_out:\", x.shape)\n",
    "\n",
    "ff_norm = nn.LayerNorm(dim)\n",
    "ff_in = ff_norm(x)\n",
    "print(\"PreNorm_ff\", ff_in.shape)\n",
    "\n",
    "hidden_dim = 2048\n",
    "ff_L1 = nn.Linear(dim, hidden_dim)\n",
    "ff = ff_L1(ff_in)\n",
    "print(\"ff_Linear1:\", ff.shape)\n",
    "\n",
    "GELU = nn.GELU()\n",
    "ff = GELU(ff)\n",
    "print(\"ff_GELU:\", ff.shape)\n",
    "\n",
    "ff_dropout1 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout1(ff)\n",
    "print(\"ff_dropout1\", ff.shape)\n",
    "\n",
    "ff_L2 = nn.Linear(hidden_dim, dim)\n",
    "ff = ff_L2(ff)\n",
    "print(\"ff_Linear2:\", ff.shape)\n",
    "\n",
    "ff_dropout2 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout2(ff)\n",
    "print(\"ff_dropout2\", ff.shape)\n",
    "\n",
    "x = ff + x\n",
    "print(\"x_ff_out:\", x.shape)\n",
    "\n",
    "x = x.mean(dim=1)\n",
    "print(\"pooling:\", x.shape)\n",
    "\n",
    "to_latent = nn.Identity()\n",
    "x = to_latent(x)\n",
    "print(\"latent:\", x.shape)\n",
    "\n",
    "generator_norm = nn.LayerNorm(dim)\n",
    "x = generator_norm(x)\n",
    "print(\"generator_norm\", x.shape)\n",
    "\n",
    "generator = nn.Linear(dim, num_pred)\n",
    "pred = generator(x)\n",
    "print(\"model_out:\", pred.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d0087ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: torch.Size([2, 40, 40, 20, 12])\n",
      "x_dim:40, y_dim:40, z_dim:20\n",
      "patch_x:40, patch_y:40, patch_z:1\n",
      "patch_reshape: torch.Size([2, 20, 19200])\n",
      "patch_Linear: torch.Size([2, 20, 512])\n",
      "pred_token: torch.Size([1, 101, 512])\n",
      "pred_tokens: torch.Size([2, 101, 512])\n",
      "cat(pred_tokens,x): torch.Size([2, 121, 512])\n",
      "pos_emb: torch.Size([2, 121, 512])\n",
      "pos_dropout: torch.Size([2, 121, 512])\n",
      "PreNorm_Attn torch.Size([2, 121, 512])\n",
      "qkv: torch.Size([2, 121, 480])\n",
      "qkv[0]: torch.Size([2, 121, 160])\n",
      "qkv[1]: torch.Size([2, 121, 160])\n",
      "qkv[2]: torch.Size([2, 121, 160])\n",
      "q: torch.Size([2, 8, 121, 20])\n",
      "k: torch.Size([2, 8, 121, 20])\n",
      "v: torch.Size([2, 8, 121, 20])\n",
      "dots: torch.Size([2, 8, 121, 121])\n",
      "attn: torch.Size([2, 8, 121, 121])\n",
      "attn_score: torch.Size([2, 8, 121, 20])\n",
      "attn_reshape torch.Size([2, 121, 160])\n",
      "attn_Linear: torch.Size([2, 121, 512])\n",
      "attn_out: torch.Size([2, 121, 512])\n",
      "x_attn_out: torch.Size([2, 121, 512])\n",
      "PreNorm_ff torch.Size([2, 121, 512])\n",
      "ff_Linear1: torch.Size([2, 121, 2048])\n",
      "ff_GELU: torch.Size([2, 121, 2048])\n",
      "ff_dropout1 torch.Size([2, 121, 2048])\n",
      "ff_Linear2: torch.Size([2, 121, 512])\n",
      "ff_dropout2 torch.Size([2, 121, 512])\n",
      "x_ff_out: torch.Size([2, 121, 512])\n",
      "spec: torch.Size([2, 7])\n",
      "spec: torch.Size([2, 1, 7])\n",
      "spec: torch.Size([2, 1, 1, 7])\n",
      "spec0_Linear:torch.Size([2, 1, 512])\n",
      "spec1_Linear:torch.Size([2, 2, 512])\n",
      "spec2_Linear:torch.Size([2, 3, 512])\n",
      "spec3_Linear:torch.Size([2, 4, 512])\n",
      "spec4_Linear:torch.Size([2, 5, 512])\n",
      "spec5_Linear:torch.Size([2, 6, 512])\n",
      "spec6_Linear:torch.Size([2, 7, 512])\n",
      "cat(x, spec_emb): torch.Size([2, 128, 512])\n",
      "pooling: torch.Size([2, 512])\n",
      "latent: torch.Size([2, 512])\n",
      "generator_norm torch.Size([2, 512])\n",
      "model_out: torch.Size([2, 101])\n"
     ]
    }
   ],
   "source": [
    "# パッチ分割 spec 完成版2\n",
    "\n",
    "patch_split_size = [1,1,20]\n",
    "\n",
    "inp = torch.rand(1,40,40,20,12)\n",
    "print(\"inp:\", inp.shape)\n",
    "\n",
    "_, x_dim, y_dim, z_dim, data_dim = inp.shape\n",
    "print(\"x_dim:%i, y_dim:%i, z_dim:%i\" % (x_dim, y_dim, z_dim))\n",
    "patch_x = x_dim // patch_split_size[0] \n",
    "patch_y = y_dim // patch_split_size[1]\n",
    "patch_z = z_dim // patch_split_size[2]\n",
    "print(\"patch_x:%i, patch_y:%i, patch_z:%i\" % (patch_x, patch_y, patch_z))\n",
    "\n",
    "num_patches = (x_dim // patch_x) * (y_dim // patch_y) * (z_dim // patch_z)\n",
    "\n",
    "patch_dim = patch_x * patch_y * patch_z * data_dim\n",
    "\n",
    "patch_reshape = Rearrange('b (x p1) (y p2) (z p3) data -> b (x y z) (p1 p2 p3 data)', p1=patch_x, p2=patch_y, p3=patch_z)  \n",
    "x = patch_reshape(inp)\n",
    "print(\"patch_reshape:\", x.shape)\n",
    "\n",
    "dim = 512\n",
    "\n",
    "patch_L = nn.Linear(patch_dim, dim)\n",
    "x = patch_L(x)\n",
    "print(\"patch_Linear:\", x.shape)\n",
    "\n",
    "b, n, _ = x.shape\n",
    "\n",
    "num_pred = 101\n",
    "\n",
    "pred_token = nn.Parameter(torch.randn(1, num_pred, dim))\n",
    "print(\"pred_token:\", pred_token.shape)\n",
    "\n",
    "pred_tokens = repeat(pred_token, '() n d -> b n d', b = b)\n",
    "print(\"pred_tokens:\", pred_tokens.shape)\n",
    "\n",
    "x = torch.cat((pred_tokens, x), dim=1)\n",
    "print(\"cat(pred_tokens,x):\", x.shape)\n",
    "\n",
    "pos_emb = nn.Parameter(torch.randn(1, num_patches + num_pred, dim))\n",
    "x += pos_emb[:,:(n+num_pred)]\n",
    "print(\"pos_emb:\", x.shape)\n",
    "\n",
    "dropout_rate = 0.1\n",
    "pos_dropout = nn.Dropout(dropout_rate)\n",
    "x = pos_dropout(x)\n",
    "print(\"pos_dropout:\", x.shape)\n",
    "\n",
    "attn_norm = nn.LayerNorm(dim)\n",
    "attn_in = attn_norm(x)\n",
    "print(\"PreNorm_Attn\", attn_in.shape)\n",
    "\n",
    "#dim_head = 512\n",
    "#heads = num_patches\n",
    "dim_head = num_patches\n",
    "heads = 8\n",
    "inner_dim = dim_head * heads\n",
    "\n",
    "scale = dim_head ** -0.5\n",
    "\n",
    "to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "print(\"qkv:\", to_qkv(x).shape)\n",
    "qkv = to_qkv(x).chunk(3, dim = -1)\n",
    "print(\"qkv[0]:\", qkv[0].shape)\n",
    "print(\"qkv[1]:\", qkv[1].shape)\n",
    "print(\"qkv[2]:\", qkv[2].shape)\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = heads), qkv)\n",
    "print(\"q:\", q.shape)\n",
    "print(\"k:\", k.shape)\n",
    "print(\"v:\", v.shape)\n",
    "\n",
    "dots = torch.matmul(q, k.transpose(-1, -2)) * scale\n",
    "print(\"dots:\", dots.shape)\n",
    "\n",
    "attend = nn.Softmax(dim = -1)\n",
    "attn = attend(dots)\n",
    "print(\"attn:\", attn.shape)\n",
    "\n",
    "attn_score = torch.matmul(attn, v)\n",
    "print(\"attn_score:\", attn_score.shape)\n",
    "\n",
    "attn_score = rearrange(attn_score, 'b h n d -> b n (h d)')\n",
    "print(\"attn_reshape\", attn_score.shape)\n",
    "\n",
    "attn_L = nn.Linear(inner_dim, dim)\n",
    "attn_out = attn_L(attn_score)\n",
    "print(\"attn_Linear:\", attn_out.shape)\n",
    "\n",
    "attn_dropout = nn.Dropout(dropout_rate)\n",
    "attn_out = attn_dropout(attn_out)\n",
    "print(\"attn_out:\", attn_out.shape)\n",
    "\n",
    "x = attn_out + x\n",
    "print(\"x_attn_out:\", x.shape)\n",
    "\n",
    "ff_norm = nn.LayerNorm(dim)\n",
    "ff_in = ff_norm(x)\n",
    "print(\"PreNorm_ff\", ff_in.shape)\n",
    "\n",
    "hidden_dim = 2048\n",
    "ff_L1 = nn.Linear(dim, hidden_dim)\n",
    "ff = ff_L1(ff_in)\n",
    "print(\"ff_Linear1:\", ff.shape)\n",
    "\n",
    "GELU = nn.GELU()\n",
    "ff = GELU(ff)\n",
    "print(\"ff_GELU:\", ff.shape)\n",
    "\n",
    "ff_dropout1 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout1(ff)\n",
    "print(\"ff_dropout1\", ff.shape)\n",
    "\n",
    "ff_L2 = nn.Linear(hidden_dim, dim)\n",
    "ff = ff_L2(ff)\n",
    "print(\"ff_Linear2:\", ff.shape)\n",
    "\n",
    "ff_dropout2 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout2(ff)\n",
    "print(\"ff_dropout2\", ff.shape)\n",
    "\n",
    "x = ff + x\n",
    "print(\"x_ff_out:\", x.shape)\n",
    "\n",
    "num_spec = 7\n",
    "spec = torch.randn(1, num_spec)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "spec = torch.unsqueeze(spec,1)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "spec = torch.unsqueeze(spec,1)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "def clones(module, n):\n",
    "    \"\"\"\n",
    "    Produce N identical layers.\n",
    "    \"\"\"\n",
    "    assert isinstance(module, nn.Module)\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(n)])\n",
    "\n",
    "spec_Ls = clones(nn.Linear(1,dim), num_spec)\n",
    "\n",
    "\n",
    "for i, spec_L in enumerate(spec_Ls):\n",
    "    if(i == 0):\n",
    "        spec_emb_all = spec_L(spec[:,:,:,i])\n",
    "        print(f'spec{i}_Linear:{spec_emb_all.shape}')\n",
    "    else:\n",
    "        spec_emb = spec_L(spec[:,:,:,i])\n",
    "        spec_emb_all = torch.cat((spec_emb_all, spec_emb), dim=1)\n",
    "        print(f'spec{i}_Linear:{spec_emb_all.shape}')\n",
    "\n",
    "\n",
    "x = torch.cat((x, spec_emb_all), dim=1)\n",
    "print(\"cat(x, spec_emb):\", x.shape)\n",
    "\n",
    "x = x.mean(dim=1)\n",
    "print(\"pooling:\", x.shape)\n",
    "\n",
    "to_latent = nn.Identity()\n",
    "x = to_latent(x)\n",
    "print(\"latent:\", x.shape)\n",
    "\n",
    "generator_norm = nn.LayerNorm(dim)\n",
    "x = generator_norm(x)\n",
    "print(\"generator_norm\", x.shape)\n",
    "\n",
    "generator = nn.Linear(dim, num_pred)\n",
    "pred = generator(x)\n",
    "print(\"model_out:\", pred.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f6b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: torch.Size([1, 40, 40, 20, 12])\n",
      "permute: torch.Size([1, 12, 40, 40, 20])\n",
      "conv1: torch.Size([1, 12, 20, 20, 10])\n",
      "conv2: torch.Size([1, 12, 10, 10, 5])\n",
      "permute: torch.Size([1, 10, 10, 5, 12])\n",
      "patch_reshape: torch.Size([1, 500, 12])\n",
      "patch_Linear: torch.Size([1, 500, 512])\n",
      "pred_token: torch.Size([1, 101, 512])\n",
      "pred_tokens: torch.Size([1, 101, 512])\n",
      "cat(pred_tokens,x): torch.Size([1, 601, 512])\n",
      "pos_emb: torch.Size([1, 601, 512])\n",
      "pos_dropout: torch.Size([1, 601, 512])\n",
      "PreNorm_Attn torch.Size([1, 601, 512])\n",
      "qkv: torch.Size([1, 601, 12000])\n",
      "qkv[0]: torch.Size([1, 601, 4000])\n",
      "qkv[1]: torch.Size([1, 601, 4000])\n",
      "qkv[2]: torch.Size([1, 601, 4000])\n",
      "q: torch.Size([1, 8, 601, 500])\n",
      "k: torch.Size([1, 8, 601, 500])\n",
      "v: torch.Size([1, 8, 601, 500])\n",
      "dots: torch.Size([1, 8, 601, 601])\n",
      "attn: torch.Size([1, 8, 601, 601])\n",
      "attn_score: torch.Size([1, 8, 601, 500])\n",
      "attn_reshape torch.Size([1, 601, 4000])\n",
      "attn_Linear: torch.Size([1, 601, 512])\n",
      "attn_out: torch.Size([1, 601, 512])\n",
      "x_attn_out: torch.Size([1, 601, 512])\n",
      "PreNorm_ff torch.Size([1, 601, 512])\n",
      "ff_Linear1: torch.Size([1, 601, 2048])\n",
      "ff_GELU: torch.Size([1, 601, 2048])\n",
      "ff_dropout1 torch.Size([1, 601, 2048])\n",
      "ff_Linear2: torch.Size([1, 601, 512])\n",
      "ff_dropout2 torch.Size([1, 601, 512])\n",
      "x_ff_out: torch.Size([1, 601, 512])\n",
      "spec: torch.Size([1, 7])\n",
      "spec: torch.Size([1, 1, 7])\n",
      "spec: torch.Size([1, 1, 1, 7])\n",
      "spec0_Linear:torch.Size([1, 1, 512])\n",
      "spec1_Linear:torch.Size([1, 2, 512])\n",
      "spec2_Linear:torch.Size([1, 3, 512])\n",
      "spec3_Linear:torch.Size([1, 4, 512])\n",
      "spec4_Linear:torch.Size([1, 5, 512])\n",
      "spec5_Linear:torch.Size([1, 6, 512])\n",
      "spec6_Linear:torch.Size([1, 7, 512])\n",
      "cat(x, spec_emb): torch.Size([1, 608, 512])\n",
      "pooling: torch.Size([1, 512])\n",
      "latent: torch.Size([1, 512])\n",
      "generator_norm torch.Size([1, 512])\n",
      "model_out: torch.Size([1, 101])\n"
     ]
    }
   ],
   "source": [
    "#convolution\n",
    "\n",
    "patch_split_size = [1,1,20]\n",
    "\n",
    "inp = torch.rand(1,40,40,20,12)\n",
    "print(\"inp:\", inp.shape)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_block(x)\n",
    "\n",
    "x = inp.permute(0, 4, 1, 2, 3)\n",
    "print(\"permute:\", x.shape)\n",
    "    \n",
    "conv1 = ConvBlock(12, 12)\n",
    "x = conv1(x)\n",
    "print(\"conv1:\", x.shape)\n",
    "\n",
    "conv2 = ConvBlock(12, 12)\n",
    "x = conv2(x)\n",
    "print(\"conv2:\", x.shape)\n",
    "\n",
    "#conv3 = ConvBlock(12, 12)\n",
    "#x = conv3(x)\n",
    "#print(\"conv3:\", x.shape)\n",
    "\n",
    "x = x.permute(0, 2, 3, 4, 1)\n",
    "print(\"permute:\", x.shape)\n",
    "\n",
    "_, x_dim, y_dim, z_dim, data_dim = x.shape\n",
    "\n",
    "num_patches = x_dim * y_dim * z_dim\n",
    "\n",
    "patch_reshape = Rearrange('b x y z data -> b (x y z) data')  \n",
    "x = patch_reshape(x)\n",
    "print(\"patch_reshape:\", x.shape)\n",
    "\n",
    "dim = 512\n",
    "\n",
    "patch_L = nn.Linear(data_dim, dim)\n",
    "x = patch_L(x)\n",
    "print(\"patch_Linear:\", x.shape)\n",
    "\n",
    "b, n, _ = x.shape\n",
    "\n",
    "num_pred = 101\n",
    "\n",
    "pred_token = nn.Parameter(torch.randn(1, num_pred, dim))\n",
    "print(\"pred_token:\", pred_token.shape)\n",
    "\n",
    "pred_tokens = repeat(pred_token, '() n d -> b n d', b = b)\n",
    "print(\"pred_tokens:\", pred_tokens.shape)\n",
    "\n",
    "x = torch.cat((pred_tokens, x), dim=1)\n",
    "print(\"cat(pred_tokens,x):\", x.shape)\n",
    "\n",
    "pos_emb = nn.Parameter(torch.randn(1, num_patches + num_pred, dim))\n",
    "x += pos_emb[:,:(n+num_pred)]\n",
    "print(\"pos_emb:\", x.shape)\n",
    "\n",
    "dropout_rate = 0.1\n",
    "pos_dropout = nn.Dropout(dropout_rate)\n",
    "x = pos_dropout(x)\n",
    "print(\"pos_dropout:\", x.shape)\n",
    "\n",
    "attn_norm = nn.LayerNorm(dim)\n",
    "attn_in = attn_norm(x)\n",
    "print(\"PreNorm_Attn\", attn_in.shape)\n",
    "\n",
    "#dim_head = 512\n",
    "#heads = num_patches\n",
    "dim_head = num_patches\n",
    "heads = 8\n",
    "inner_dim = dim_head * heads\n",
    "\n",
    "scale = dim_head ** -0.5\n",
    "\n",
    "to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "print(\"qkv:\", to_qkv(x).shape)\n",
    "qkv = to_qkv(x).chunk(3, dim = -1)\n",
    "print(\"qkv[0]:\", qkv[0].shape)\n",
    "print(\"qkv[1]:\", qkv[1].shape)\n",
    "print(\"qkv[2]:\", qkv[2].shape)\n",
    "q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = heads), qkv)\n",
    "print(\"q:\", q.shape)\n",
    "print(\"k:\", k.shape)\n",
    "print(\"v:\", v.shape)\n",
    "\n",
    "dots = torch.matmul(q, k.transpose(-1, -2)) * scale\n",
    "print(\"dots:\", dots.shape)\n",
    "\n",
    "attend = nn.Softmax(dim = -1)\n",
    "attn = attend(dots)\n",
    "print(\"attn:\", attn.shape)\n",
    "\n",
    "attn_score = torch.matmul(attn, v)\n",
    "print(\"attn_score:\", attn_score.shape)\n",
    "\n",
    "attn_score = rearrange(attn_score, 'b h n d -> b n (h d)')\n",
    "print(\"attn_reshape\", attn_score.shape)\n",
    "\n",
    "attn_L = nn.Linear(inner_dim, dim)\n",
    "attn_out = attn_L(attn_score)\n",
    "print(\"attn_Linear:\", attn_out.shape)\n",
    "\n",
    "attn_dropout = nn.Dropout(dropout_rate)\n",
    "attn_out = attn_dropout(attn_out)\n",
    "print(\"attn_out:\", attn_out.shape)\n",
    "\n",
    "x = attn_out + x\n",
    "print(\"x_attn_out:\", x.shape)\n",
    "\n",
    "ff_norm = nn.LayerNorm(dim)\n",
    "ff_in = ff_norm(x)\n",
    "print(\"PreNorm_ff\", ff_in.shape)\n",
    "\n",
    "hidden_dim = 2048\n",
    "ff_L1 = nn.Linear(dim, hidden_dim)\n",
    "ff = ff_L1(ff_in)\n",
    "print(\"ff_Linear1:\", ff.shape)\n",
    "\n",
    "GELU = nn.GELU()\n",
    "ff = GELU(ff)\n",
    "print(\"ff_GELU:\", ff.shape)\n",
    "\n",
    "ff_dropout1 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout1(ff)\n",
    "print(\"ff_dropout1\", ff.shape)\n",
    "\n",
    "ff_L2 = nn.Linear(hidden_dim, dim)\n",
    "ff = ff_L2(ff)\n",
    "print(\"ff_Linear2:\", ff.shape)\n",
    "\n",
    "ff_dropout2 = nn.Dropout(dropout_rate)\n",
    "ff = ff_dropout2(ff)\n",
    "print(\"ff_dropout2\", ff.shape)\n",
    "\n",
    "x = ff + x\n",
    "print(\"x_ff_out:\", x.shape)\n",
    "\n",
    "num_spec = 7\n",
    "spec = torch.randn(1, num_spec)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "spec = torch.unsqueeze(spec,1)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "spec = torch.unsqueeze(spec,1)\n",
    "print(\"spec:\", spec.shape)\n",
    "\n",
    "def clones(module, n):\n",
    "    \"\"\"\n",
    "    Produce N identical layers.\n",
    "    \"\"\"\n",
    "    assert isinstance(module, nn.Module)\n",
    "    return nn.ModuleList([deepcopy(module) for _ in range(n)])\n",
    "\n",
    "spec_Ls = clones(nn.Linear(1,dim), num_spec)\n",
    "\n",
    "\n",
    "for i, spec_L in enumerate(spec_Ls):\n",
    "    if(i == 0):\n",
    "        spec_emb_all = spec_L(spec[:,:,:,i])\n",
    "        print(f'spec{i}_Linear:{spec_emb_all.shape}')\n",
    "    else:\n",
    "        spec_emb = spec_L(spec[:,:,:,i])\n",
    "        spec_emb_all = torch.cat((spec_emb_all, spec_emb), dim=1)\n",
    "        print(f'spec{i}_Linear:{spec_emb_all.shape}')\n",
    "\n",
    "\n",
    "x = torch.cat((x, spec_emb_all), dim=1)\n",
    "print(\"cat(x, spec_emb):\", x.shape)\n",
    "\n",
    "x = x.mean(dim=1)\n",
    "print(\"pooling:\", x.shape)\n",
    "\n",
    "to_latent = nn.Identity()\n",
    "x = to_latent(x)\n",
    "print(\"latent:\", x.shape)\n",
    "\n",
    "generator_norm = nn.LayerNorm(dim)\n",
    "x = generator_norm(x)\n",
    "print(\"generator_norm\", x.shape)\n",
    "\n",
    "generator = nn.Linear(dim, num_pred)\n",
    "pred = generator(x)\n",
    "print(\"model_out:\", pred.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcdc46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
